{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries and modules\n",
    "import nltk \n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Lemmatizer - transform words into base form - jumping = jump\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt'))\n",
    "\n",
    "#load reviews \n",
    "positive_reviews = BeautifulSoup(open('positive.review').read())\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('negative.review').read())\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are more positive reviews than negative reviews => class imbalance => shuffle positive and delete to have same range as negative \n",
    "np.random.shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    s = s.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "\n",
    "\n",
    "#create word map index: \n",
    "word_map_index = {}\n",
    "current_index = 0 \n",
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens: \n",
    "        if token not in word_map_index: \n",
    "            word_map_index[token] = current_index\n",
    "            current_index +=1\n",
    "            \n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens: \n",
    "        if token not in word_map_index: \n",
    "            word_map_index[token] = current_index\n",
    "            current_index +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dangngochuy/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate:  0.69\n"
     ]
    }
   ],
   "source": [
    "# take each token and create an array of numbers \n",
    "def tokens_to_vector(tokens,label):\n",
    "    x = np.zeros(len(word_map_index)+1)\n",
    "    for t in tokens: \n",
    "        i = word_map_index[t]\n",
    "        x[i] +=1\n",
    "    x = x/x.sum()\n",
    "    x[-1] = label\n",
    "    return x\n",
    "\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "\n",
    "data = np.zeros((N, len(word_map_index) + 1))\n",
    "i = 0 \n",
    "for tokens in positive_tokenized: \n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i +=1\n",
    "for tokens in negative_tokenized: \n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i +=1\n",
    "\n",
    "np.random.shuffle(data)\n",
    "X = data[:, :-1]\n",
    "Y = data[:, -1]\n",
    "\n",
    "X_train = X[:-100,]\n",
    "Y_train = Y[:-100,]\n",
    "X_test = X[-100:,]\n",
    "Y_test = Y[-100:,]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,Y_train)\n",
    "print(\"Classification rate: \", model.score(X_test, Y_test))\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "then -1.0530318332939599\n",
      "try -0.6655543298093445\n",
      "month -0.7304452979295445\n",
      "customer -0.7028073495823965\n",
      "wa -1.7141353233052259\n",
      "bit 0.6148295650763265\n",
      "price 2.666124444699022\n",
      "little 0.9457783276909697\n",
      "n't -1.9983609003381035\n",
      "happy 0.6456968393719918\n",
      "cable 0.6746659302892987\n",
      "quality 1.413367532659135\n",
      "perfect 1.0018262461800205\n",
      "video 0.5539990353420746\n",
      "'ve 0.7955454799005883\n",
      "support -0.906694175945994\n",
      "week -0.6594152536382365\n",
      "sound 1.2572562124841644\n",
      "comfortable 0.6624848324036833\n",
      "doe -1.3139065428054573\n",
      "card -0.632126403949423\n",
      "lot 0.551475428183394\n",
      "love 1.2264589545125453\n",
      "ha 0.6869590230181629\n",
      "speaker 0.9234256506862881\n",
      "poor -0.7701178775649423\n",
      "bad -0.7748307656174696\n",
      "you 0.897126650038981\n",
      "buy -0.9265717057038769\n",
      "time -0.6047039664102574\n",
      "hour -0.5966367395050187\n",
      "using 0.7513983634825856\n",
      "home 0.5121466390874151\n",
      "space 0.6080725836080594\n",
      "unit -0.9127356792965857\n",
      "easy 1.8029240888552984\n",
      "excellent 1.3827570753875917\n",
      "recommend 0.6545909388674771\n",
      "pretty 0.7231977722314218\n",
      "memory 0.8049618563259299\n",
      "stopped -0.5508275602410151\n",
      "paper 0.6003536371963881\n",
      "tried -0.8108350397910582\n",
      "look 0.532993220900213\n",
      "fast 0.882552874225602\n",
      "money -1.1133719612577984\n",
      "warranty -0.6183229969303702\n",
      "static -0.5117989473479417\n",
      "sent -0.5477123826437816\n",
      "highly 0.9686592555069057\n",
      "value 0.5121374291264934\n",
      "item -0.96505117735252\n",
      "returned -0.8128318776580122\n",
      "company -0.5442393967325211\n",
      "waste -0.9382615162417531\n",
      "returning -0.5282514555404125\n",
      "return -1.17700508804488\n",
      "refund -0.6544967795970661\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "for word, index in word_map_index.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold: \n",
    "        print(word, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
